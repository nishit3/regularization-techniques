# regularization-techniques
## Dropout probability experiment
I have assessed and plotted the train and test accuracy of the ANN-based binary classification model in relation to the dropout probabilities. When the dropout probability (p) increased, both accuracies decreased and overfitting increased. The conclusion that can be drawn is that thisÂ model performs best when there are no dropouts.  

![Figure_1](https://github.com/nishit3/regularization-techniques/assets/90385616/a695b10d-2e0f-40b8-92e7-c6b32bccbd29)

## Dropout probability experiment on Iris dataset
![Figure_1](https://github.com/nishit3/regularization-techniques/assets/90385616/0c64918b-de89-4e2d-88eb-65df263a57d0)

## Ridge/Weight Decay/L2 Regularization Parameter Experiment on Iris Dataset
![Figure_1](https://github.com/nishit3/regularization-techniques/assets/90385616/02be70da-62c8-4b30-a2b6-752e224b4599)

## Lasso/L1 Regularization Parameter Experiment on Iris Dataset
![Figure_1](https://github.com/nishit3/regularization-techniques/assets/90385616/221302f8-2f38-44e6-8f2c-5858dd8d0ccc)

## ElasticNet Regularization Parameter Experiment on Iris Dataset
![Figure_1](https://github.com/nishit3/regularization-techniques/assets/90385616/3afbb676-c3d9-447c-af21-2b07c8f4a9c1)

## Accuracy as a function of mini-batch size
![Figure_1](https://github.com/nishit3/regularization-techniques/assets/90385616/b9ffe133-1f06-410f-80ea-685fca9ea2da)   

![Figure_2](https://github.com/nishit3/regularization-techniques/assets/90385616/6a550687-d0eb-4913-818b-dd91c4a852f3)

## With vs Without Batch Normalization on ANN-based Binary Classifier (Wine Taste Good/Bad)
![with batchnorm](https://github.com/nishit3/regularization-techniques/assets/90385616/0cae57ba-60ec-4c0d-9990-4a96a9b573c8)

![without batchnorm](https://github.com/nishit3/regularization-techniques/assets/90385616/1d5aaf8b-d7e1-41ca-9c4b-3da4efdf9d67)
